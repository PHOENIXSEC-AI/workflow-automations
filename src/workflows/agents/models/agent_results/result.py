"""
Pydantic models for agent execution results.

This module defines strongly typed models for the results returned by agent tasks.
These models provide a structured way to handle both successful and failed agent executions,
as well as batch processing results.
"""
import uuid
from typing import Optional, List, Union, Dict, Any
from pydantic import BaseModel, Field
from datetime import datetime

from .base import AgentAnalysisResult


class TokenUsage(BaseModel):
    """
    Model representing token usage metrics for an LLM call.
    
    This tracks the number of tokens used in prompts and completions,
    which is useful for monitoring API usage and costs.
    """
    prompt: Optional[int] = Field(
        default=None,
        description="Number of tokens used in the prompt"
    )
    completion: Optional[int] = Field(
        default=None,
        description="Number of tokens used in the completion"
    )
    total: Optional[int] = Field(
        default=None,
        description="Total number of tokens used"
    )


class AgentSuccessResult(BaseModel):
    """
    Model representing a successful agent execution.
    
    Contains the full details of a completed agent task, including performance
    metrics and the actual result content.
    """
    task: 'AgentTask' = Field(
        default=None,
        description="Task that was executed"
    )
    is_success: bool = Field(
        default=True,
        description="Status of the agent execution"
    )
    result: Union[str, AgentAnalysisResult] = Field(
        description="The content returned by the LLM"
    )
    duration_seconds: float = Field(
        description="Duration of the agent execution in seconds"
    )
    agent: str = Field(
        description="Name of the agent that was used"
    )
    model: Optional[str] = Field(
        default=None,
        description="Name of the LLM model that was used"
    )
    tokens: Optional[TokenUsage] = Field(
        default=None,
        description="Token usage information"
    )
    prompt_index: Optional[int] = Field(
        default=None,
        description="Index of the prompt in a batch execution"
    )
    timestamp: datetime = Field(
        default_factory=datetime.now,
        description="Timestamp when the result was created"
    )


class AgentErrorResult(BaseModel):
    """
    Model representing a failed agent execution.
    
    Contains detailed error information to help diagnose and address issues
    that occurred during agent execution.
    """
    task: 'AgentTask' = Field(
        default=None,
        description="Task that was executed"
    )
    is_success: bool = Field(
        default=False,
        description="Status of the agent execution"
    )
    error_type: str = Field(
        description="Type of error that occurred"
    )
    message: str = Field(
        description="Error message"
    )
    status_code: Optional[int] = Field(
        default=None,
        description="HTTP status code (for API errors)"
    )
    exception_type: Optional[str] = Field(
        default=None,
        description="Type of exception that was raised"
    )
    prompt_index: Optional[int] = Field(
        default=None,
        description="Index of the prompt in a batch execution"
    )
    timestamp: datetime = Field(
        default_factory=datetime.now,
        description="Timestamp when the error occurred"
    )


class AgentBatchResult(BaseModel):
    """
    Model representing the results of a batch of agent executions.
    
    Provides summary statistics and a collection of individual results
    from processing multiple tasks in one batch.
    """
    total_tasks: int = Field(
        default=0,
        description="Total number of tasks in the batch"
    )
    successful: int = Field(
        default=0,
        description="Number of successful tasks"
    )
    failed: int = Field(
        default=0,
        description="Number of failed tasks"
    )
    results: List[Union['AgentSuccessResult', 'AgentErrorResult']] = Field(
        default_factory=list,
        description="List of individual task results"
    )
    def get_file_map(self) -> Dict[str,Union['AgentSuccessResult', 'AgentErrorResult']]:
        return {result_item.get('file_path','default'): result_item for result_item in self.results}
    

class DBOpsResult(BaseModel):
    """
    Model for the completed merge operation of an AI analysis run.
    
    Links together the task information, unique artifact identifier,
    and the actual analysis results from the agent.
    """
    task: 'AgentTask' = Field(
        description="Information about the task that was run"
    )
    artifact_id: Optional[uuid.UUID] = Field(
        default=None,
        description="Unique identifier for the artifact generated by this analysis"
    )
    result: AgentAnalysisResult = Field(
        description="The analysis results from the agent"
    )

class AgentTask(BaseModel):
    """
    Model representing a task for an agent.
    
    Contains the instructions and context needed for an agent to perform
    a specific analysis or operation.
    """
    instructions: str = Field(
        description="The formatted prompt content to send to the LLM"
    )
    repo_name: Optional[str] = Field(
        default="",
        description="Name of the repository being analyzed"
    )
    file_path: Optional[str] = Field(
        default=None,
        description="Path of the file being analyzed, if applicable"
    )


# Union type for agent results
AgentResult = Union[AgentSuccessResult, AgentErrorResult]

# Update forward references
AgentSuccessResult.model_rebuild()
AgentErrorResult.model_rebuild()
DBOpsResult.model_rebuild()
AgentBatchResult.model_rebuild()

__all__ = [
    "TokenUsage",
    "AgentSuccessResult",
    "AgentErrorResult",
    "AgentResult",
    "AgentBatchResult",
    "AgentTask",
    "DBOpsResult",
] 